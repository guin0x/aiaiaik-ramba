{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install h5py\n",
    "# !pip install pandas matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scipy\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import copy\n",
    "import h5py\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from model.model_3D import *\n",
    "from model.train_eval import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from postprocessing.save_results import *\n",
    "from postprocessing.plot_results import *\n",
    "from postprocessing.metrics import single_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "### check if cuda is available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import NDVI data\n",
    "\n",
    "def load_with_hdf5(name):\n",
    "    \"\"\"\n",
    "    Load tensor data from HDF5 with gzip compression\n",
    "    \"\"\"\n",
    "    with h5py.File(f\"data/{name}.h5\", \"r\") as f:\n",
    "        loaded_features = torch.from_numpy(f[\"features\"][:])\n",
    "        loaded_labels = torch.from_numpy(f[\"labels\"][:])\n",
    "    return TensorDataset(loaded_features, loaded_labels)\n",
    "\n",
    "train_set = load_with_hdf5(\"train_set_ndvi_v7\")\n",
    "val_set = load_with_hdf5(\"val_set_ndvi_v7\")\n",
    "test_set = load_with_hdf5(\"test_set_ndvi_v7\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 518 samples\n",
      "Validation set: 19 samples\n",
      "Test set: 19 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {len(train_set)} samples\")\n",
    "print(f\"Validation set: {len(val_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([518, 2, 4, 1000, 500])\n",
      "torch.Size([518, 1000, 500])\n"
     ]
    }
   ],
   "source": [
    "print(train_set.tensors[0].shape)\n",
    "print(train_set.tensors[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(batch_size, learning_rate, init_hid_dim, num_epochs=100, device='cuda'):\n",
    "    # Initialize model\n",
    "    model = UNet3D(\n",
    "        n_channels=train_set[0][0].shape[0],\n",
    "        n_classes=1,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        kernel_size=3,\n",
    "        pooling='max',\n",
    "        bilinear=False,\n",
    "        drop_channels=False\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {num_parameters:.2e}.\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=15, gamma=0.75)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    csi_scores = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training step\n",
    "        train_loss = training_unet(model, train_loader, optimizer, device=device, loss_f='BCE', water_threshold=0.5)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(\n",
    "            model, val_loader, device=device, loss_f='BCE', water_threshold=0.5\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        accuracies.append(val_accuracy)\n",
    "        precisions.append(val_precision)\n",
    "        recalls.append(val_recall)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        csi_scores.append(val_csi_score)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        # Print progress every 20 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch: {epoch} | Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                  f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "            print(f\"Metrics | Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f}, \"\n",
    "                  f\"F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}\")\n",
    "            print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    return best_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for Batch Size=16, Learning Rate=0.05, Init Hid Dim=32\n",
      "Number of parameters: 7.77e+06.\n",
      "Epoch: 1 | Training Loss: 0.2500, Validation Loss: 2.8002, Best Validation Loss: 2.8002\n",
      "Metrics | Accuracy: 0.898, Precision: 0.516, Recall: 0.349, F1-score: 0.416, CSI-score: 0.263\n",
      "Learning Rate: 0.050000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.05_hid32_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          3.124e+00\n",
      "Accuracy:          0.901\n",
      "Precision:         0.608\n",
      "Recall:            0.395\n",
      "F1 score:          0.479\n",
      "CSI score:         0.315\n"
     ]
    }
   ],
   "source": [
    "# Test first with Antonio's Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.05\n",
    "init_hid_dim = 32\n",
    "num_epochs = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_f = 'BCE'\n",
    "machine = 'machine_1'\n",
    "\n",
    "# Train and validate\n",
    "print(f\"Running training for Batch Size={batch_size}, Learning Rate={learning_rate}, Init Hid Dim={init_hid_dim}\")\n",
    "val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores = train_and_validate(\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    init_hid_dim=init_hid_dim,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the best model\n",
    "save_model_path(\n",
    "    machine=machine,\n",
    "    model=best_model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    init_hid_dim=init_hid_dim,\n",
    "    epochs=num_epochs,\n",
    "    dir_output=\"model/models_trained\"\n",
    ")\n",
    "\n",
    "# Save training and validation metrics\n",
    "save_losses_metrics(\n",
    "    machine=machine,\n",
    "    train_losses=train_losses,\n",
    "    val_losses=val_losses,\n",
    "    metrics=[accuracies, precisions, recalls, f1_scores, csi_scores],\n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    init_hid_dim=init_hid_dim, \n",
    "    epochs=num_epochs,\n",
    "    dir_output=\"model/losses_metrics\" \n",
    ")\n",
    "\n",
    "# Define the test loader\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the best model\n",
    "model_loss = copy.deepcopy(best_model)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(\n",
    "    model_loss, test_loader, device=device, loss_f=loss_f\n",
    ")\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "BCE loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Batch size=16, Learning rate=0.05, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.2352, Validation Loss: 0.2369, Best Validation Loss: 0.2369\n",
      "Metrics | Accuracy: 0.910, Precision: 0.620, Recall: 0.368, F1-score: 0.462, CSI-score: 0.300\n",
      "Learning Rate: 0.050000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.05_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          2.411e-01\n",
      "Accuracy:          0.910\n",
      "Precision:         0.697\n",
      "Recall:            0.385\n",
      "F1 score:          0.496\n",
      "CSI score:         0.330\n",
      "Finished: Batch Size=16, Learning Rate=0.05, Init Hid Dim=16, Val Loss=0.23689965903759003, Test Loss=0.2411319613456726\n",
      "Testing: Batch size=16, Learning rate=0.05, Init Hid Dim=32\n",
      "Number of parameters: 7.77e+06.\n",
      "Epoch: 1 | Training Loss: 0.2178, Validation Loss: 25.7143, Best Validation Loss: 25.7143\n",
      "Metrics | Accuracy: 0.641, Precision: 0.224, Recall: 0.989, F1-score: 0.365, CSI-score: 0.223\n",
      "Learning Rate: 0.050000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.05_hid32_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          2.484e+01\n",
      "Accuracy:          0.659\n",
      "Precision:         0.250\n",
      "Recall:            0.988\n",
      "F1 score:          0.400\n",
      "CSI score:         0.250\n",
      "Finished: Batch Size=16, Learning Rate=0.05, Init Hid Dim=32, Val Loss=25.71428680419922, Test Loss=24.840835571289062\n",
      "Testing: Batch size=16, Learning rate=0.1, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.2735, Validation Loss: 0.1928, Best Validation Loss: 0.1928\n",
      "Metrics | Accuracy: 0.911, Precision: 0.562, Recall: 0.695, F1-score: 0.622, CSI-score: 0.451\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.1_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          1.924e-01\n",
      "Accuracy:          0.913\n",
      "Precision:         0.600\n",
      "Recall:            0.736\n",
      "F1 score:          0.661\n",
      "CSI score:         0.494\n",
      "Finished: Batch Size=16, Learning Rate=0.1, Init Hid Dim=8, Val Loss=0.19275003671646118, Test Loss=0.19240090250968933\n",
      "Testing: Batch size=16, Learning rate=0.1, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.2458, Validation Loss: 1.9539, Best Validation Loss: 1.9539\n",
      "Metrics | Accuracy: 0.895, Precision: 0.000, Recall: 0.000, F1-score: 0.000, CSI-score: 0.000\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.1_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          2.341e+00\n",
      "Accuracy:          0.885\n",
      "Precision:         0.000\n",
      "Recall:            0.000\n",
      "F1 score:          0.000\n",
      "CSI score:         0.000\n",
      "Finished: Batch Size=16, Learning Rate=0.1, Init Hid Dim=16, Val Loss=1.9538769721984863, Test Loss=2.3405721187591553\n",
      "Testing: Batch size=16, Learning rate=0.1, Init Hid Dim=32\n",
      "Number of parameters: 7.77e+06.\n",
      "Epoch: 1 | Training Loss: 0.2146, Validation Loss: 1.2556, Best Validation Loss: 1.2556\n",
      "Metrics | Accuracy: 0.723, Precision: 0.272, Recall: 0.982, F1-score: 0.426, CSI-score: 0.270\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs16_lr0.1_hid32_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          1.326e+00\n",
      "Accuracy:          0.721\n",
      "Precision:         0.289\n",
      "Recall:            0.984\n",
      "F1 score:          0.447\n",
      "CSI score:         0.288\n",
      "Finished: Batch Size=16, Learning Rate=0.1, Init Hid Dim=32, Val Loss=1.2555959224700928, Test Loss=1.3258004188537598\n",
      "Testing: Batch size=32, Learning rate=0.01, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.3862, Validation Loss: 0.4010, Best Validation Loss: 0.4010\n",
      "Metrics | Accuracy: 0.859, Precision: 0.427, Recall: 0.897, F1-score: 0.579, CSI-score: 0.407\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs32_lr0.01_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          3.865e-01\n",
      "Accuracy:          0.870\n",
      "Precision:         0.465\n",
      "Recall:            0.921\n",
      "F1 score:          0.618\n",
      "CSI score:         0.447\n",
      "Finished: Batch Size=32, Learning Rate=0.01, Init Hid Dim=8, Val Loss=0.4010469317436218, Test Loss=0.3864954113960266\n",
      "Testing: Batch size=32, Learning rate=0.01, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.4618, Validation Loss: 5.1097, Best Validation Loss: 5.1097\n",
      "Metrics | Accuracy: 0.892, Precision: 0.432, Recall: 0.001, F1-score: 0.002, CSI-score: 0.001\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/losses_metrics_NDVI_bs32_lr0.01_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "BCE loss:          5.642e+00\n",
      "Accuracy:          0.885\n",
      "Precision:         0.377\n",
      "Recall:            0.001\n",
      "F1 score:          0.002\n",
      "CSI score:         0.001\n",
      "Finished: Batch Size=32, Learning Rate=0.01, Init Hid Dim=16, Val Loss=5.109740734100342, Test Loss=5.6416916847229\n",
      "Testing: Batch size=32, Learning rate=0.01, Init Hid Dim=32\n",
      "Number of parameters: 7.77e+06.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 803.50 MiB is free. Process 2652186 has 5.95 GiB memory in use. Process 90521 has 37.59 GiB memory in use. Of the allocated memory 35.66 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting: Batch size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Learning rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Init Hid Dim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minit_hid_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_hid_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_hid_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m     36\u001b[0m save_model_path(\n\u001b[1;32m     37\u001b[0m     best_model,\n\u001b[1;32m     38\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     dir_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/models_trained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m )\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(batch_size, learning_rate, init_hid_dim, num_epochs, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBCE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwater_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Validation step\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/train_eval.py:72\u001b[0m, in \u001b[0;36mtraining_unet\u001b[0;34m(model, loader, optimizer, nonwater, water, pixel_size, water_threshold, device, loss_f)\u001b[0m\n\u001b[1;32m     68\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert target tensor to float\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# get predictions\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# compute binary classification loss\u001b[39;00m\n\u001b[1;32m     75\u001b[0m binary_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(predictions, target)\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/train_eval.py:24\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model, input_dataset, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(model, input_dataset, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Compute the predictions given the deep-learning model class and input dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m           predictions = list, model predictions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m    \n\u001b[0;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:138\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x3)\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x2)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutc(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:82\u001b[0m, in \u001b[0;36mUp.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     79\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffX \u001b[38;5;241m-\u001b[39m diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     80\u001b[0m                 diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffY \u001b[38;5;241m-\u001b[39m diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x2, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:43\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 803.50 MiB is free. Process 2652186 has 5.95 GiB memory in use. Process 90521 has 37.59 GiB memory in use. Of the allocated memory 35.66 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "# Define hyperparameter values\n",
    "batch_sizes = [8, 16, 32]\n",
    "learning_rates = [0.01, 0.05, 0.1]\n",
    "init_hid_dims = [8, 16, 32]\n",
    "num_epochs = 1\n",
    "loss_f = 'BCE'  # Define loss function for training and testing\n",
    "machine='machine_1'\n",
    "\n",
    "# Split combinations across machines\n",
    "all_combinations = list(itertools.product(batch_sizes, learning_rates, init_hid_dims))\n",
    "split_index = len(all_combinations) // 2\n",
    "combinations_machine_1 = all_combinations[:split_index]\n",
    "combinations_machine_2 = all_combinations[split_index:]\n",
    "\n",
    "# Select the current machine's combinations\n",
    "combinations = combinations_machine_2  # Change to combinations_machine_1 on Machine 1\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Run grid search\n",
    "for batch_size, learning_rate, init_hid_dim in combinations:\n",
    "    print(f\"Testing: Batch size={batch_size}, Learning rate={learning_rate}, Init Hid Dim={init_hid_dim}\")\n",
    "    \n",
    "    # Train and validate\n",
    "    val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores = train_and_validate(\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Save the best model\n",
    "    save_model_path(\n",
    "        machine=machine,\n",
    "        model=best_model,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/models_trained\"\n",
    "    )\n",
    "    \n",
    "    # Save training and validation metrics\n",
    "    save_losses_metrics(\n",
    "        machine=machine,\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        metrics=[accuracies, precisions, recalls, f1_scores, csi_scores],\n",
    "        batch_size=batch_size, \n",
    "        learning_rate=learning_rate, \n",
    "        init_hid_dim=init_hid_dim, \n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/losses_metrics\" \n",
    "    )\n",
    "\n",
    "    # Define the test loader\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Test the best model on the test dataset\n",
    "    model_loss = copy.deepcopy(best_model)\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(\n",
    "        model_loss, test_loader, device=device, loss_f=loss_f\n",
    "    )\n",
    "    \n",
    "    print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "{loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')\n",
    "    \n",
    "    # Append results\n",
    "    results.append((\n",
    "        batch_size, learning_rate, init_hid_dim, val_loss, test_loss, test_accuracy, \n",
    "        test_precision, test_recall, test_f1_score, test_csi_score\n",
    "    ))\n",
    "    print(f\"Finished: Batch Size={batch_size}, Learning Rate={learning_rate}, \"\n",
    "          f\"Init Hid Dim={init_hid_dim}, Val Loss={val_loss}, Test Loss={test_loss}\")\n",
    "\n",
    "# Save results to a CSV file\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    'Batch Size', 'Learning Rate', 'Init Hid Dim', \n",
    "    'Validation Loss', 'Test Loss', 'Test Accuracy', \n",
    "    'Test Precision', 'Test Recall', 'Test F1 Score', 'Test CSI Score'\n",
    "])\n",
    "df_results.to_csv(\"results_machine_1.csv\", index=False)\n",
    "print(\"Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
