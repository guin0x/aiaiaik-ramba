{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install h5py\n",
    "# !pip install pandas matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install scipy\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "import copy\n",
    "import h5py\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "from model.model_3D import *\n",
    "from model.train_eval import *\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from postprocessing.save_results import *\n",
    "from postprocessing.plot_results import *\n",
    "from postprocessing.metrics import single_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda.\n"
     ]
    }
   ],
   "source": [
    "### check if cuda is available\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import NDVI data\n",
    "\n",
    "def load_with_hdf5(name):\n",
    "    \"\"\"\n",
    "    Load tensor data from HDF5 with gzip compression\n",
    "    \"\"\"\n",
    "    with h5py.File(f\"data/{name}.h5\", \"r\") as f:\n",
    "        loaded_features = torch.from_numpy(f[\"features\"][:])\n",
    "        loaded_labels = torch.from_numpy(f[\"labels\"][:])\n",
    "    return TensorDataset(loaded_features, loaded_labels)\n",
    "\n",
    "train_set = load_with_hdf5(\"train_set_ndvi_v7\")\n",
    "val_set = load_with_hdf5(\"val_set_ndvi_v7\")\n",
    "test_set = load_with_hdf5(\"test_set_ndvi_v7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 518 samples\n",
      "Validation set: 19 samples\n",
      "Test set: 19 samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set: {len(train_set)} samples\")\n",
    "print(f\"Validation set: {len(val_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([518, 2, 4, 1000, 500])\n",
      "torch.Size([518, 1000, 500])\n"
     ]
    }
   ],
   "source": [
    "print(train_set.tensors[0].shape)\n",
    "print(train_set.tensors[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stats():\n",
    "    print((torch.cuda.memory_allocated()/1024**2), \"Memory allocated\")\n",
    "    print(torch.cuda.memory_cached()/1024**2, \"Memory cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(batch_size, learning_rate, init_hid_dim, num_epochs=100, device='cuda'):\n",
    "    # Initialize model\n",
    "    model = UNet3D(\n",
    "        n_channels=train_set[0][0].shape[0],\n",
    "        n_classes=1,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        kernel_size=3,\n",
    "        pooling='max',\n",
    "        bilinear=False,\n",
    "        drop_channels=False\n",
    "    )\n",
    "\n",
<<<<<<< Updated upstream
    "    torch.cuda.empty_cache()\n",
    "\n",
=======
    "    # Clear CUDA cache and run garbage collection before training loop\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
>>>>>>> Stashed changes
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {num_parameters:.2e}.\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=15, gamma=0.75)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    csi_scores = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training step\n",
    "        train_loss = training_unet(model, train_loader, optimizer, device=device, loss_f='BCE', water_threshold=0.5)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(\n",
    "            model, val_loader, device=device, loss_f='BCE', water_threshold=0.5\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        accuracies.append(val_accuracy)\n",
    "        precisions.append(val_precision)\n",
    "        recalls.append(val_recall)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        csi_scores.append(val_csi_score)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        # Print progress every 20 epochs\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch: {epoch} | Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                  f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "            print(f\"Metrics | Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f}, \"\n",
    "                  f\"F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}\")\n",
    "            print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    del optimizer\n",
    "    del val_loss\n",
    "    del model\n",
    "    del train_loader\n",
    "    del train_loss\n",
    "    \n",
    "    return best_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first with Antonio's Hyperparameters\n",
    "batch_size = 16\n",
    "learning_rate = 0.05\n",
    "init_hid_dim = 8\n",
    "num_epochs = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "loss_f = 'BCE'\n",
<<<<<<< Updated upstream
    "machine = 'machine_1'\n",
=======
    "machine= 'machine_1'\n",
>>>>>>> Stashed changes
    "\n",
    "# Train and validate\n",
    "print(f\"Running training for Batch Size={batch_size}, Learning Rate={learning_rate}, Init Hid Dim={init_hid_dim}\")\n",
    "val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores = train_and_validate(\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    init_hid_dim=init_hid_dim,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the best model\n",
    "save_model_path(\n",
    "    machine=machine,\n",
    "    model=best_model,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    init_hid_dim=init_hid_dim,\n",
    "    epochs=num_epochs,\n",
    "    dir_output=\"model/models_trained\"\n",
    ")\n",
    "\n",
    "# Save training and validation metrics\n",
    "save_losses_metrics(\n",
    "    machine=machine,\n",
    "    train_losses=train_losses,\n",
    "    val_losses=val_losses,\n",
    "    metrics=[accuracies, precisions, recalls, f1_scores, csi_scores],\n",
    "    batch_size=batch_size, \n",
    "    learning_rate=learning_rate, \n",
    "    init_hid_dim=init_hid_dim, \n",
    "    epochs=num_epochs,\n",
    "    dir_output=\"model/losses_metrics\" \n",
    ")\n",
    "\n",
    "# Define the test loader\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test the best model\n",
    "model_loss = copy.deepcopy(best_model)\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(\n",
    "    model_loss, test_loader, device=device, loss_f=loss_f\n",
    ")\n",
    "\n",
    "print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "BCE loss:          {test_loss:.3e}\\n\\\n",
    "Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "Precision:         {test_precision:.3f}\\n\\\n",
    "Recall:            {test_recall:.3f}\\n\\\n",
    "F1 score:          {test_f1_score:.3f}\\n\\\n",
    "CSI score:         {test_csi_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Batch size=8, Learning rate=0.01, Init Hid Dim=8\n",
      "Unable to reset GPU 00000000:D2:00.0: Insufficient Permissions\n",
      "0.0 Memory allocated\n",
      "0.0 Memory cached\n",
      "Number of parameters: 4.87e+05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.2006, Validation Loss: 0.1640, Best Validation Loss: 0.1640\n",
      "Metrics | Accuracy: 0.924, Precision: 0.662, Recall: 0.586, F1-score: 0.622, CSI-score: 0.451\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs8_lr0.1_hid32_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          1.572e-01\n",
      "            Accuracy:          0.927\n",
      "            Precision:         0.695\n",
      "            Recall:            0.644\n",
      "            F1 score:          0.669\n",
      "            CSI score:         0.503\n",
      "Finished: Batch Size=8, Learning Rate=0.1, Init Hid Dim=32, Val Loss=0.1640099287033081, Test Loss=0.15722306072711945\n",
      "Testing: Batch size=32, Learning rate=0.01, Init Hid Dim=8\n",
      "Unable to reset GPU 00000000:D2:00.0: Insufficient Permissions\n",
      "61.1640625 Memory allocated\n",
      "140.0 Memory cached\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.4348, Validation Loss: 2.7527, Best Validation Loss: 2.7527\n",
      "Metrics | Accuracy: 0.564, Precision: 0.198, Recall: 0.991, F1-score: 0.330, CSI-score: 0.197\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs32_lr0.01_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          2.734e+00\n",
      "            Accuracy:          0.584\n",
      "            Precision:         0.215\n",
      "            Recall:            0.990\n",
      "            F1 score:          0.353\n",
      "            CSI score:         0.214\n",
      "Finished: Batch Size=32, Learning Rate=0.01, Init Hid Dim=8, Val Loss=2.752723217010498, Test Loss=2.733942747116089\n",
      "Testing: Batch size=32, Learning rate=0.01, Init Hid Dim=32\n",
      "Unable to reset GPU 00000000:D2:00.0: Insufficient Permissions\n",
      "3.802734375 Memory allocated\n",
      "12.0 Memory cached\n",
      "Number of parameters: 7.77e+06.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 1.85 GiB is free. Process 2990717 has 638.00 MiB memory in use. Process 3107055 has 41.85 GiB memory in use. Of the allocated memory 41.38 GiB is allocated by PyTorch, and 154.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m memory_stats()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_hid_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_hid_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m save_model_path(machine\u001b[38;5;241m=\u001b[39mmachine,\n\u001b[1;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mbest_model,\n\u001b[1;32m     43\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     dir_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/models_trained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(batch_size, learning_rate, init_hid_dim, num_epochs, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBCE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwater_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Validation step\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/train_eval.py:72\u001b[0m, in \u001b[0;36mtraining_unet\u001b[0;34m(model, loader, optimizer, nonwater, water, pixel_size, water_threshold, device, loss_f)\u001b[0m\n\u001b[1;32m     68\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert target tensor to float\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# get predictions\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# compute binary classification loss\u001b[39;00m\n\u001b[1;32m     75\u001b[0m binary_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(predictions, target)\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/train_eval.py:24\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(model, input_dataset, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(model, input_dataset, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Compute the predictions given the deep-learning model class and input dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m           predictions = list, model predictions\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m    \n\u001b[0;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:138\u001b[0m, in \u001b[0;36mUNet3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x3)\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup3(x, x2)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutc(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:82\u001b[0m, in \u001b[0;36mUp.forward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     79\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x1, [diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffX \u001b[38;5;241m-\u001b[39m diffX \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     80\u001b[0m                 diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, diffY \u001b[38;5;241m-\u001b[39m diffY \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     81\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x2, x1], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/aiaiaik-ramba/model/model_3D.py:43\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.91 GiB. GPU 0 has a total capacty of 44.34 GiB of which 1.85 GiB is free. Process 2990717 has 638.00 MiB memory in use. Process 3107055 has 41.85 GiB memory in use. Of the allocated memory 41.38 GiB is allocated by PyTorch, and 154.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "# Define hyperparameter values\n",
    "batch_sizes = [8, 32]\n",
    "learning_rates = [0.01, 0.1]\n",
    "init_hid_dims = [8, 32]\n",
    "num_epochs = 1\n",
    "loss_f = 'BCE'  # Define loss function for training and testing\n",
<<<<<<< Updated upstream
    "machine='machine_1'\n",
=======
    "machine = 'machine_1'\n",
>>>>>>> Stashed changes
    "\n",
    "# Split combinations across machines\n",
    "all_combinations = list(itertools.product(batch_sizes, learning_rates, init_hid_dims))\n",
    "split_index = len(all_combinations) // 2\n",
    "combinations_machine_1 = all_combinations[:split_index]\n",
    "combinations_machine_2 = all_combinations[split_index:]\n",
    "\n",
    "# Select the current machine's combinations\n",
    "combinations = combinations_machine_1  # Change to combinations_machine_1 on Machine 1\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Run grid search\n",
    "for batch_size, learning_rate, init_hid_dim in all_combinations:\n",
    "    print(f\"Testing: Batch size={batch_size}, Learning rate={learning_rate}, Init Hid Dim={init_hid_dim}\")\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    memory_stats()\n",
    "    \n",
    "    # Train and validate\n",
    "    val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores = train_and_validate(\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Save the best model\n",
<<<<<<< Updated upstream
    "    save_model_path(\n",
    "        machine=machine,\n",
=======
    "    save_model_path(machine=machine,\n",
>>>>>>> Stashed changes
    "        model=best_model,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/models_trained\"\n",
    "    )\n",
    "    \n",
    "    # Save training and validation metrics\n",
<<<<<<< Updated upstream
    "    save_losses_metrics(\n",
    "        machine=machine,\n",
=======
    "    save_losses_metrics(machine=machine,\n",
>>>>>>> Stashed changes
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        metrics=[accuracies, precisions, recalls, f1_scores, csi_scores],\n",
    "        batch_size=batch_size, \n",
    "        learning_rate=learning_rate, \n",
    "        init_hid_dim=init_hid_dim, \n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/losses_metrics\" \n",
    "    )\n",
    "\n",
    "    # Define the test loader\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Test the best model on the test dataset\n",
    "    model_loss = copy.deepcopy(best_model)\n",
    "    test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(\n",
    "        model_loss, test_loader, device=device, loss_f=loss_f\n",
    "    )\n",
    "    \n",
    "    print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "            {loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "            Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "            Precision:         {test_precision:.3f}\\n\\\n",
    "            Recall:            {test_recall:.3f}\\n\\\n",
    "            F1 score:          {test_f1_score:.3f}\\n\\\n",
    "            CSI score:         {test_csi_score:.3f}')\n",
    "    \n",
    "    # Append results\n",
    "    results.append((\n",
    "        batch_size, learning_rate, init_hid_dim, val_loss, test_loss, test_accuracy, \n",
    "        test_precision, test_recall, test_f1_score, test_csi_score\n",
    "    ))\n",
    "    print(f\"Finished: Batch Size={batch_size}, Learning Rate={learning_rate}, \"\n",
    "          f\"Init Hid Dim={init_hid_dim}, Val Loss={val_loss}, Test Loss={test_loss}\")\n",
    "    \n",
    "\n",
    "# Save results to a CSV file\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    'Batch Size', 'Learning Rate', 'Init Hid Dim', \n",
    "    'Validation Loss', 'Test Loss', 'Test Accuracy', \n",
    "    'Test Precision', 'Test Recall', 'Test F1 Score', 'Test CSI Score'\n",
    "])\n",
    "df_results.to_csv(f\"results_{machine}.csv\", index=False)\n",
    "print(\"Results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: Batch size=8, Learning rate=0.01, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.2828, Validation Loss: 0.2214, Best Validation Loss: 0.2214\n",
      "Metrics | Accuracy: 0.920, Precision: 0.607, Recall: 0.726, F1-score: 0.661, CSI-score: 0.494\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs8_lr0.01_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          2.182e-01\n",
      "            Accuracy:          0.924\n",
      "            Precision:         0.640\n",
      "            Recall:            0.772\n",
      "            F1 score:          0.700\n",
      "            CSI score:         0.538\n",
      "Testing: Batch size=8, Learning rate=0.01, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.2328, Validation Loss: 0.1731, Best Validation Loss: 0.1731\n",
      "Metrics | Accuracy: 0.918, Precision: 0.592, Recall: 0.748, F1-score: 0.661, CSI-score: 0.493\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs8_lr0.01_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          1.649e-01\n",
      "            Accuracy:          0.925\n",
      "            Precision:         0.640\n",
      "            Recall:            0.788\n",
      "            F1 score:          0.706\n",
      "            CSI score:         0.546\n",
      "Testing: Batch size=8, Learning rate=0.1, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.2135, Validation Loss: 0.1945, Best Validation Loss: 0.1945\n",
      "Metrics | Accuracy: 0.909, Precision: 0.550, Recall: 0.811, F1-score: 0.656, CSI-score: 0.488\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs8_lr0.1_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          1.966e-01\n",
      "            Accuracy:          0.913\n",
      "            Precision:         0.583\n",
      "            Recall:            0.857\n",
      "            F1 score:          0.694\n",
      "            CSI score:         0.531\n",
      "Testing: Batch size=8, Learning rate=0.1, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.2040, Validation Loss: 0.1929, Best Validation Loss: 0.1929\n",
      "Metrics | Accuracy: 0.909, Precision: 0.553, Recall: 0.797, F1-score: 0.653, CSI-score: 0.485\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs8_lr0.1_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          1.891e-01\n",
      "            Accuracy:          0.913\n",
      "            Precision:         0.585\n",
      "            Recall:            0.839\n",
      "            F1 score:          0.689\n",
      "            CSI score:         0.526\n",
      "Testing: Batch size=16, Learning rate=0.01, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.4632, Validation Loss: 0.6168, Best Validation Loss: 0.6168\n",
      "Metrics | Accuracy: 0.822, Precision: 0.366, Recall: 0.949, F1-score: 0.528, CSI-score: 0.359\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs16_lr0.01_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          6.192e-01\n",
      "            Accuracy:          0.828\n",
      "            Precision:         0.397\n",
      "            Recall:            0.960\n",
      "            F1 score:          0.562\n",
      "            CSI score:         0.391\n",
      "Testing: Batch size=16, Learning rate=0.01, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n",
      "Epoch: 1 | Training Loss: 0.3530, Validation Loss: 0.2089, Best Validation Loss: 0.2089\n",
      "Metrics | Accuracy: 0.913, Precision: 0.559, Recall: 0.784, F1-score: 0.653, CSI-score: 0.485\n",
      "Learning Rate: 0.010000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs16_lr0.01_hid16_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          2.015e-01\n",
      "            Accuracy:          0.917\n",
      "            Precision:         0.603\n",
      "            Recall:            0.819\n",
      "            F1 score:          0.695\n",
      "            CSI score:         0.532\n",
      "Testing: Batch size=16, Learning rate=0.1, Init Hid Dim=8\n",
      "Number of parameters: 4.87e+05.\n",
      "Epoch: 1 | Training Loss: 0.2193, Validation Loss: 0.4498, Best Validation Loss: 0.4498\n",
      "Metrics | Accuracy: 0.835, Precision: 0.384, Recall: 0.948, F1-score: 0.546, CSI-score: 0.376\n",
      "Learning Rate: 0.100000\n",
      "Metrics saved at: model/losses_metrics/machine_1_losses_metrics_NDVI_bs16_lr0.1_hid8_epoch1.csv\n",
      "Average metrics for test dataset using model with best validation loss:\n",
      "\n",
      "            BCE loss:          4.371e-01\n",
      "            Accuracy:          0.848\n",
      "            Precision:         0.429\n",
      "            Recall:            0.956\n",
      "            F1 score:          0.592\n",
      "            CSI score:         0.421\n",
      "Testing: Batch size=16, Learning rate=0.1, Init Hid Dim=16\n",
      "Number of parameters: 1.95e+06.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "def train_and_validate(batch_size, learning_rate, init_hid_dim, num_epochs=100, device='cuda'):\n",
    "    # Initialize model\n",
    "    model = UNet3D(\n",
    "        n_channels=train_set[0][0].shape[0],\n",
    "        n_classes=1,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        kernel_size=3,\n",
    "        pooling='max',\n",
    "        bilinear=False,\n",
    "        drop_channels=False\n",
    "    )\n",
    "\n",
    "    # Clear CUDA cache and run garbage collection before training loop\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    num_parameters = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Number of parameters: {num_parameters:.2e}.\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=15, gamma=0.75)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    csi_scores = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Training step\n",
    "        train_loss = training_unet(model, train_loader, optimizer, device=device, loss_f='BCE', water_threshold=0.5)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():  # Prevent gradient computation in validation\n",
    "            val_loss, val_accuracy, val_precision, val_recall, val_f1_score, val_csi_score = validation_unet(\n",
    "                model, val_loader, device=device, loss_f='BCE', water_threshold=0.5\n",
    "            )\n",
    "\n",
    "        # Move metrics to CPU and append to lists\n",
    "        val_losses.append(val_loss)\n",
    "        accuracies.append(val_accuracy)\n",
    "        precisions.append(val_precision)\n",
    "        recalls.append(val_recall)\n",
    "        f1_scores.append(val_f1_score)\n",
    "        csi_scores.append(val_csi_score)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = copy.deepcopy(model).cpu()  # Move model to CPU for storage\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch: {epoch} | Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, \"\n",
    "                  f\"Best Validation Loss: {best_loss:.4f}\")\n",
    "            print(f\"Metrics | Accuracy: {val_accuracy:.3f}, Precision: {val_precision:.3f}, Recall: {val_recall:.3f}, \"\n",
    "                  f\"F1-score: {val_f1_score:.3f}, CSI-score: {val_csi_score:.3f}\")\n",
    "            print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del optimizer, val_loss, model, train_loader, train_loss\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return best_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "batch_sizes = [8, 16]\n",
    "learning_rates = [0.01, 0.1]\n",
    "init_hid_dims = [8, 16]\n",
    "num_epochs = 1\n",
    "loss_f = 'BCE'  # Define loss function for training and testing\n",
    "machine = 'machine_1'\n",
    "\n",
    "# Split combinations across machines\n",
    "all_combinations = list(itertools.product(batch_sizes, learning_rates, init_hid_dims))\n",
    "split_index = len(all_combinations) // 2\n",
    "combinations_machine_1 = all_combinations[:split_index]\n",
    "combinations_machine_2 = all_combinations[split_index:]\n",
    "\n",
    "# Select the current machine's combinations\n",
    "combinations = combinations_machine_1\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "# Run grid search\n",
    "for batch_size, learning_rate, init_hid_dim in all_combinations:\n",
    "    print(f\"Testing: Batch size={batch_size}, Learning rate={learning_rate}, Init Hid Dim={init_hid_dim}\")\n",
    "\n",
    "    # Clear CUDA memory and garbage collect\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Train and validate\n",
    "    val_loss, best_model, train_losses, val_losses, accuracies, precisions, recalls, f1_scores, csi_scores = train_and_validate(\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Save the best model\n",
    "    save_model_path(machine=machine,\n",
    "        model=best_model,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        init_hid_dim=init_hid_dim,\n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/models_trained\"\n",
    "    )\n",
    "\n",
    "    # Save training and validation metrics\n",
    "    save_losses_metrics(machine=machine,\n",
    "        train_losses=train_losses,\n",
    "        val_losses=val_losses,\n",
    "        metrics=[accuracies, precisions, recalls, f1_scores, csi_scores],\n",
    "        batch_size=batch_size, \n",
    "        learning_rate=learning_rate, \n",
    "        init_hid_dim=init_hid_dim, \n",
    "        epochs=num_epochs,\n",
    "        dir_output=\"model/losses_metrics\" \n",
    "    )\n",
    "\n",
    "    # Define the test loader\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Test the best model on the test dataset\n",
    "    with torch.no_grad():\n",
    "        test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_csi_score = validation_unet(\n",
    "            best_model, test_loader, device=device, loss_f=loss_f\n",
    "        )\n",
    "\n",
    "    print(f'Average metrics for test dataset using model with best validation loss:\\n\\n\\\n",
    "            {loss_f} loss:          {test_loss:.3e}\\n\\\n",
    "            Accuracy:          {test_accuracy:.3f}\\n\\\n",
    "            Precision:         {test_precision:.3f}\\n\\\n",
    "            Recall:            {test_recall:.3f}\\n\\\n",
    "            F1 score:          {test_f1_score:.3f}\\n\\\n",
    "            CSI score:         {test_csi_score:.3f}')\n",
    "\n",
    "    # Append results\n",
    "    results.append((\n",
    "        batch_size, learning_rate, init_hid_dim, \n",
    "        float(val_loss), float(test_loss), \n",
    "        float(test_accuracy), float(test_precision), float(test_recall), \n",
    "        float(test_f1_score), float(test_csi_score)\n",
    "    ))\n",
    "\n",
    "    # Cleanup\n",
    "    del best_model, test_loader\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save results to a CSV file\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    'Batch Size', 'Learning Rate', 'Init Hid Dim', \n",
    "    'Validation Loss', 'Test Loss', 'Test Accuracy', \n",
    "    'Test Precision', 'Test Recall', 'Test F1 Score', 'Test CSI Score'\n",
    "])\n",
    "df_results.to_csv(f\"results_{machine}.csv\", index=False)\n",
    "print(\"Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
